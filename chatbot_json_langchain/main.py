import os
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from loader import carregar_documentos_json
from embedding_store import criar_chroma, carregar_chroma

load_dotenv()

def criar_chatbot():
    persist_dir = "chroma_db"
    
    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
        print("ðŸ”„ Criando base vetorial...")
        documentos = carregar_documentos_json("dados/conteudofinal.json")
        vectorstore = criar_chroma(documentos, persist_dir)
    else:
        print("âœ… Carregando base vetorial existente...")
        vectorstore = carregar_chroma(persist_dir)

    retriever = vectorstore.as_retriever()
    llm = ChatOpenAI(model_name="gpt-3.5-turbo")
    chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)
    
    return chain

if __name__ == "__main__":
    chatbot = criar_chatbot()
    
    while True:
        pergunta = input("FaÃ§a uma pergunta: ")
        if pergunta.lower() in ["sair", "exit", "quit"]:
            break
        resposta = chatbot(pergunta)
        print("\nðŸ¤– Bot:", resposta["result"])
        fontes = [doc.metadata['titulo'] for doc in resposta["source_documents"]]
        print("ðŸ“š Fontes:", fontes)
        print("-" * 50)
